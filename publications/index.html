<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Eva Guttmann-Flury </title> <meta name="author" content="Eva Guttmann-Flury"> <meta name="description" content="Much Ado About Publications"> <meta name="keywords" content="Brain-Computer Interface (BCI), Electroencephalography (EEG), Artifact removal, Sample size calculation, Source Localization"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/brain_icon8.png?804ea0a6ce96a82945fd53d763263dd1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qinxinlan.github.io/publications/"> <script src="/assets/js/theme.js?7b1068a1099d4262cace6933e385240d"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?46af317e693b09921dcb92261d123fbc" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eva</span> Guttmann-Flury </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/projects/">Overview</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/1_project/">Blinks</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/2_project/">Algorithms</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/3_project/">Variability</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/4_project/">Statistics</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/zh/publications/"> ZH</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Much Ado About Publications</h1> </header> <article> <script src="/assets/js/bibsearch.js?8470e568bd8f7fe1a4046936bc17c40d" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">Journal Articles</h2> <h3 class="bibliography">2022</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Behav Res</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guttmannflury2022css-480.webp 480w,/assets/img/publication_preview/guttmannflury2022css-800.webp 800w,/assets/img/publication_preview/guttmannflury2022css-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guttmannflury2022css.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guttmannflury2022css.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guttmannflury2022css" class="col-sm-8"> <div class="title">Channel selection from source localization: A review of four EEG-based brain–computer interfaces paradigms</div> <div class="author"> <em>E. Guttmann-Flury</em>, <a href="https://scholar.google.com/citations?user=A1ENH7gAAAAJ" rel="external nofollow noopener" target="_blank">X. Sheng</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=MjW-8-cAAAAJ" rel="external nofollow noopener" target="_blank">X. Zhu</a> </div> <div class="periodical"> <em>Behavior Research Methods</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/publication/361819223_Channel_selection_from_source_localization_A_review_of_four_EEG-based_brain-computer_interfaces_paradigms" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/guttmannflury2022css.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Channel selection is a critical part of the classification procedure for multichannel electroencephalogram (EEG)-based brain–computer interfaces (BCI). An optimized subset of electrodes reduces computational complexity and optimizes accuracy. Different tasks activate different sources in the brain and are characterized by distinctive channels. The goal of the current review is to define a subset of electrodes for each of four popular BCI paradigms: motor imagery, motor execution, steady-state visual evoked potentials and P300. Twenty-one studies have been reviewed to identify the most significant activations of cortical sources. The relevant EEG sensors are determined from the reported 3D Talairach coordinates. They are scored by their weighted mean Cohen’s d and its confidence interval, providing the magnitude of the corresponding effect size and its statistical significance. Our goal is to create a knowledge-based channel selection framework with a sufficient statistical power. The core channel selection (CCS) could be used as a reference by EEG researchers and would have the advantages of practicality and rapidity, allowing for an easy implementation of semiparametric algorithms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guttmannflury2022css</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Channel selection from source localization: A review of four EEG-based brain–computer interfaces paradigms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guttmann-Flury, E. and Sheng, X. and Zhu, X.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Behavior Research Methods}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{55}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1980–2003}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3758/s13428-022-01897-2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.3758/s13428-022-01897-2}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Channel selection, Electroencephalography, EEG, Brain computer interface, BCI, Motor imagery, Motor execution, SSVEP, P300}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">2019</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Comput Biol Med</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guttmannflury2019abc-480.webp 480w,/assets/img/publication_preview/guttmannflury2019abc-800.webp 800w,/assets/img/publication_preview/guttmannflury2019abc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guttmannflury2019abc.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guttmannflury2019abc.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guttmannflury2019abc" class="col-sm-8"> <div class="title">A new algorithm for blink correction adaptive to inter- and intra-subject variability</div> <div class="author"> <em>E. Guttmann-Flury</em>, <a href="https://scholar.google.com/citations?user=A1ENH7gAAAAJ" rel="external nofollow noopener" target="_blank">X. Sheng</a>, <a href="https://scholar.google.com/citations?user=59fHPRsAAAAJ" rel="external nofollow noopener" target="_blank">D. Zhang</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=MjW-8-cAAAAJ" rel="external nofollow noopener" target="_blank">X. Zhu</a> </div> <div class="periodical"> <em>Computers in Biology and Medicine</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.academia.edu/40785466/A_new_algorithm_for_blink_correction_adaptive_to_inter_and_intra_subject_variability" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/guttmannflury2019abc.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Electroencephalographic (EEG) signals are constantly superimposed with biological artifacts. In particular, spontaneous blinks represent a recurrent event that cannot be easily avoided. The main goal of this paper is to present a new algorithm for blink correction (ABC) that is adaptive to inter- and intra-subject variability. The whole process of designing a Brain-Computer Interface (BCI)-based EEG experiment is highlighted. From sample size determination to classification, a mixture of the standardized low-resolution electromagnetic tomography (sLORETA) for source localization and time restriction, followed by Riemannian geometry classifiers is featured. Comparison between ABC and the commonly-used Independent Component Analysis (ICA) for blinks removal shows a net amelioration with ABC. With the same pipeline using uncorrected data as a reference, ABC improves classification by 5.38% in average, whereas ICA deteriorates by −2.67%. Furthermore, while ABC accurately reconstructs blink-free data from simulated data, ICA yields a potential difference up to 200% from the original blink-free signal and an increased variance of 30.42%. Finally, ABC’s major advantages are ease of visualization and understanding, low computation load favoring simple real-time implementation, and lack of spatial filtering, which allows for more flexibility during the classification step.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">guttmannflury2019abc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A new algorithm for blink correction adaptive to inter- and intra-subject variability}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guttmann-Flury, E. and Sheng, X. and Zhang, D. and Zhu, X.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers in Biology and Medicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{114}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103442}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0010-4825}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.compbiomed.2019.103442}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/abs/pii/S0010482519303191}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Electroencephalography, EEG, Brain computer interface, BCI, Spontaneous blinking, Artifact removal, Sample size calculation, Standardized low-resolution electromagnetic tomography, sLORETA, Riemannian geometry, Independent component analysis, ICA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">2005</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="IWASAKI2005878" class="col-sm-8"> <div class="title">Effects of eyelid closure, blinks, and eye movements on the electroencephalogram</div> <div class="author"> Masaki Iwasaki, Christoph Kellinghaus, Andreas V. Alexopoulos, Richard C. Burgess, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Arun N. Kumar, Yanning H. Han, Hans O. Lüders, R. John Leigh' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Clinical Neurophysiology</em>, 2005 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Objective To characterize the effects of the eyeball and eyelid positions during eyeblinks on electroencephalographic (EEG) potentials. Methods Movements of the upper eyelids and eyes were measured in two healthy subjects using the magnetic search coil technique during horizontal and vertical eye rotations, eyeblinks, and lid closure. Corresponding signal changes were recorded simultaneously on the electroencephalogram (EEG). Results Spontaneous blinks produced small eye movements directed down and inward, whereas slow or forced blinks were associated with delayed upward eye rotations (i.e. Bell’s phenomenon); both types of blinks caused positive EEG potentials with bifrontal distribution maximum at Fp1 and Fp2. Conclusions In prior reports, these positive EEG artifacts have been attributed to upward eyeball rotation during blinks—Bell’s phenomenon. By contrast, our findings indicate that movements of the eyelid contribute to a greater extent to these EEG potentials than do upward eyeball rotations. Significance Care is required in attributing EEG artifacts to movements of either eyeball or eyelid, since our findings suggest that they both contribute to these potentials.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">Conference Articles</h2> <h3 class="bibliography">2019</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMBC 2019</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guttmannflury2019apss-480.webp 480w,/assets/img/publication_preview/guttmannflury2019apss-800.webp 800w,/assets/img/publication_preview/guttmannflury2019apss-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guttmannflury2019apss.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guttmannflury2019apss.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guttmannflury2019apss" class="col-sm-8"> <div class="title">A Priori Sample Size Determination for the Number of Subjects in an EEG Experiment</div> <div class="author"> <em>E. Guttmann-Flury</em>, <a href="https://scholar.google.com/citations?user=A1ENH7gAAAAJ" rel="external nofollow noopener" target="_blank">X. Sheng</a>, <a href="https://scholar.google.com/citations?user=59fHPRsAAAAJ" rel="external nofollow noopener" target="_blank">D. Zhang</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=MjW-8-cAAAAJ" rel="external nofollow noopener" target="_blank">X. Zhu</a> </div> <div class="periodical"> <em>In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Eva-Guttmann-Flury/publication/333045167_A_Priori_Sample_Size_Determination_for_the_Number_of_Subjects_in_an_EEG_Experiment/links/5cd947a592851c4eab9a40b3/A-Priori-Sample-Size-Determination-for-the-Number-of-Subjects-in-an-EEG-Experiment.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/guttmannflury2019apss.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper represents a first attempt to perform a priori sample size determination from a "historic" Electroencephalography (EEG) dataset. The importance of adequate sample size is firstly highlighted, and evidence is given against the use of normal distribution for such computations, when the data cannot be assumed to be Gaussian. The "historic" dataset is then thoroughly examined to determine the least less likely underlying distribution for the desired phenomenon, in this case the spontaneous blinks potential. Two Monte Carlo simulations, using different distribution assumptions, are subsequently computed to estimate the a priori minimum sample size. Finally, these choices are discussed considering practical limitations, as well as the computational differences for other phenomena to study.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">guttmannflury2019apss</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Priori Sample Size Determination for the Number of Subjects in an EEG Experiment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guttmann-Flury, E. and Sheng, X. and Zhang, D. and Zhu, X.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5180-5183}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/EMBC.2019.8857482}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/8857482}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Electroencephalography, Gaussian distribution, Monte Carlo methods, Brain modeling, Computational modeling, Visualization}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NER 2019</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guttmannflury2019prabc-480.webp 480w,/assets/img/publication_preview/guttmannflury2019prabc-800.webp 800w,/assets/img/publication_preview/guttmannflury2019prabc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guttmannflury2019prabc.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guttmannflury2019prabc.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guttmannflury2019prabc" class="col-sm-8"> <div class="title">Preliminary Results on a New Algorithm for Blink Correction Adaptive to Inter- and Intra-Subject Variability</div> <div class="author"> <em>E. Guttmann-Flury</em>, <a href="https://scholar.google.com/citations?user=A1ENH7gAAAAJ" rel="external nofollow noopener" target="_blank">X. Sheng</a>, <a href="https://scholar.google.com/citations?user=59fHPRsAAAAJ" rel="external nofollow noopener" target="_blank">D. Zhang</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=MjW-8-cAAAAJ" rel="external nofollow noopener" target="_blank">X. Zhu</a> </div> <div class="periodical"> <em>In 2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1910.14292" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/guttmannflury2019prabc.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents a new preprocessing method to correct blinking artifacts in Electroencephalography (EEG) based Brain-Computer Interfaces (BCIs). This Algorithm for Blink Correction (ABC) directly corrects the signal in the time domain without the need for additional Electrooculogram (EOG) electrodes. The main idea is to automatically adapt to the blink’s inter- and intra-subject variability by considering the blink’s amplitude as a parameter. A simple Minimum Distance to Riemannian Mean (MDRM) is applied as the classification algorithm. Preliminary results on three subjects show a mean classification accuracy increase of 13.7% using ABC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">guttmannflury2019prabc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Preliminary Results on a New Algorithm for Blink Correction Adaptive to Inter- and Intra-Subject Variability}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guttmann-Flury, E. and Sheng, X. and Zhang, D. and Zhu, X.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{585-588}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/NER.2019.8717029}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/8717029}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Electroencephalography, Covariance matrices, Electrodes, Electrooculography, Classification algorithms}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">Unpublished</h2> <h3 class="bibliography">2024</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Sci Data</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guttmannflury2024dataset-480.webp 480w,/assets/img/publication_preview/guttmannflury2024dataset-800.webp 800w,/assets/img/publication_preview/guttmannflury2024dataset-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guttmannflury2024dataset.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guttmannflury2024dataset.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guttmannflury2024dataset" class="col-sm-8"> <div class="title">Dataset combining EEG, eye-tracking, and high-speed video for ocular activity analysis across BCI paradigms</div> <div class="author"> <em>E. Guttmann-Flury</em>, <a href="https://scholar.google.com/citations?user=A1ENH7gAAAAJ" rel="external nofollow noopener" target="_blank">X. Sheng</a>, and <a href="https://scholar.google.com/citations?hl=en&amp;user=MjW-8-cAAAAJ" rel="external nofollow noopener" target="_blank">X. Zhu</a> </div> <div class="periodical"> Apr 2024 </div> <div class="periodical"> Submitted to Scientific Data </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In Brain-Computer Interface (BCI) research, the detailed study of blinks is crucial, as they significantly affect the efficiency and accuracy derived from various common paradigms, altering the decoding of users’ cognitive states and intentions. We introduce a large dataset that captures electroencephalogram (EEG) signals and incorporates eye-tracking, high-speed camera recordings, as well as subjects’ state of mind and characteristics, to provide a multifactor analysis of eye-related movements. Four distinct paradigms – motor imagery, motor execution, steady-state visually evoked potentials, and P300 spellers – are selected due to their capacity to evoke various sensory-motor responses and potential influence on ocular activity. The online-available dataset comprises over 46 hours of data collected from 31 subjects across 63 sessions, resulting in 2520 trials for each of the first three paradigms, and 5670 trials for P300. This multimodal and multi-paradigms dataset is expected to allow the development of algorithms capable of efficiently handling eye-induced artifacts and enhancing task-specific classification. Furthermore, it offers the opportunity to evaluate the cross-paradigm robustness involving the same participants.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@unpublished</span><span class="p">{</span><span class="nl">guttmannflury2024dataset</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dataset combining EEG, eye-tracking, and high-speed video for ocular activity analysis across BCI paradigms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guttmann-Flury, E. and Sheng, X. and Zhu, X.}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Submitted to Scientific Data}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Brain-Computer Interfaces, Neuroscience, Cognitive science}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Sigradi 2024</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yang2024sigradi-480.webp 480w,/assets/img/publication_preview/yang2024sigradi-800.webp 800w,/assets/img/publication_preview/yang2024sigradi-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/yang2024sigradi.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yang2024sigradi.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024sigradi" class="col-sm-8"> <div class="title">EEG emotion recognition from AI-generated biodigital architecture images</div> <div class="author"> H. Yang, and <em>E. Guttmann-Flury</em> </div> <div class="periodical"> Jul 2024 </div> <div class="periodical"> Accepted to XXVIII Conference of the Ibero-American Society of Digital Graphics </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Emotional responses to biodigital architecture were examined using electroencephalographic (EEG) data from AI-generated images. A pre-experiment involving 336 participants identified 60 images, selected from an initial pool of 600, that elicited strong emotional responses categorized as awe, disgust, or content. These images were used for EEG recordings of 52 volunteers, with channel selection and sample size estimation based on the analysis of an existing dataset. Gamma and delta bands yielded the highest classification accuracy, with the gamma band achieving 77.07% ± 13.8% accuracy for the awe emotion. Key factors such as greenery and non-uniform granularity were linked to positive emotions, while dampness triggered negative reactions. These results emphasize the significance of incorporating natural elements and varied textures in biodigital architecture to enhance aesthetic appeal and acceptance. The study demonstrates EEG’s capability to objectively assess architectural preferences, providing valuable insights for architects to design engaging and sustainable environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@unpublished</span><span class="p">{</span><span class="nl">yang2024sigradi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EEG emotion recognition from AI-generated biodigital architecture images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, H. and Guttmann-Flury, E.}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Accepted to XXVIII Conference of the Ibero-American Society of Digital Graphics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Biodigital architecture, Emotional classification, Electroencephalogram, EEG, Al-generated images, Deep learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">Theses</h2> <h3 class="bibliography">2023</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SJTU</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/guttmannflury2023thesis-480.webp 480w,/assets/img/publication_preview/guttmannflury2023thesis-800.webp 800w,/assets/img/publication_preview/guttmannflury2023thesis-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/guttmannflury2023thesis.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="guttmannflury2023thesis.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guttmannflury2023thesis" class="col-sm-8"> <div class="title">Innovative artefact elimination and source localization-based feature extraction for EEG BCI pipelines</div> <div class="author"> <em>E. Guttmann-Flury</em> </div> <div class="periodical"> <em>Shanghai Jiao Tong University</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/guttmannflury2023thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p> </p> <p> In the thriving realm of neuroscience, Brain-Computer Interfaces (BCI) have emerged as a groundbreaking technology that facilitates direct communication between the human brain and external devices or applications. By harnessing the power of neural signals, BCI have the potential to revolutionize various domains, including healthcare, assistive technology, and human-computer interaction. Multiple recording modalities are available for capturing brain activity. Among them, electroencephalography (EEG) devices stand out as a wearable, comfortable, and cost-effective solution that provides high temporal resolution to non-invasively monitor basic user intentions. </p> <br><p> This research is dedicated to the rigorous investigation of motor imagery, with a specific focus on discerning patterns associated with grasping movements of the right and left hands. The ultimate objective is to achieve the highest possible classification accuracy of user intent without the need for neurofeedback or additional indirect information. The limitations of EEG in terms of signal-to-noise ratio and spatial resolution necessitate the use of source localization techniques, which can unveil the intricate dynamic interactions and connections within the brain. To easily distinguish between existing algorithms and the new ones introduced in this research, the original acronyms and appellations will be noted in bold. </p> <br><p> Ensuring the accuracy and reliability of EEG data analysis requires the homogeneity of electrode functioning and the cleanliness of signals in the vast majority of cases. This entails addressing two primary objectives: eliminating statistically significant (and consequential) artifacts and rectifying any malfunctioning electrodes. The spontaneous blink represents a major physiological disturbance, occurring simultaneously with the neural signal of interest with an average probability of 10%. </p> <br><p> These considerations led to the design of a multi-modal dataset targeted at comprehensively recording every aspect of the blinks using EEG, electrooculography, electromyography, eye tracker, and high-speed camera. Additional paradigms, including motor execution, P300, and steady-state visual evoked potentials have also been investigated due to their relevance in inferring a comprehensive understanding of the underlying neural mechanisms during MI. </p> <br><p> The Fitted Distribution Monte Carlo (FDMC) simulation is first conducted for a priori sample size estimation during a prospective power analysis. The goal is to ensure that sufficient data will be collected to truly comprehend the intricacies of the cortical phenomenon. FDMC demonstrates its superiority by lowering the sample size requirement by 35% compared to the Normal distribution (and by seven times compared to traditional power tables). FDMC is of particular interest when only limited resources or time are available. </p> <br><p> The resulting multi-modal dataset facilitated the development of a robust model for accurately classifying and correcting blink artifacts as well as removing bad channels from the EEG data. The Adaptive Blink Correction and De-drifting (ABCD) algorithm has proved to improve the overall data quality and displays significantly better results than the state-of-the-art, i.e., Independent Component Analysis (ICA), or Artifact Subspace Reconstruction (ASR). The classification accuracy, along with its confidence interval at 95% confidence level, reveals a mean classification accuracy of 93.81% [74.81%; 98.76%] for ABCD against 79.29% [57.41%; 92.89%] for ICA or 84.05% [62.88%; 95.31%] for ASR. </p> <br><p> A combination of various innovative algorithms is implemented to extract the most prominent signal of interest (SOI) across the temporal, spatial, and frequential domains. Source localization is first computed on the ABCDcleaned EEG data. The Source Localized Spatio-Temporal (SLST) features approach is applied to capture the spatio-temporal characteristics of the SOI by analyzing the similarities across multiple trials. To further enhance the classification process, a Dual Classifier (DC) is implemented, utilizing both the spatial locations and time-derived covariance matrices from the SLST feature extraction algorithm. The class-specific Fréchet means, along with all other covariance matrices, are computed at the Core Channel Selection, which is determined based on a comprehensive meta-analysis. </p> <br><p> Comparisons across different frequency bands (delta, theta, alpha, and beta), filter types (Butterworth, Chebyshev, and Elliptic), and re-referencing, e.g., Common Average Reference, Large and Small Laplacian spatial filters, are also carried out to reveal the optimized combination of all these processing steps. To ensure a fair comparison, all data are first pre-processed with ABCD, re-referenced with the Large Laplacian filter and filtered with a beta pass-band Butterworth filter, as this combination of various filters proved to yield the best accuracy. </p> <br><p> A new introduced Consistency computation serves to validate classifiers’ stability and can thus be used to confirm the hierarchy between them, revealing that our SLST+DC method yields a mean classification accuracy of 95.03% (SD = 3.41%). The commonly employed Common Spatial Pattern (CSP) coupled with Linear Discriminant Analysis (LDA) yields 89.16% (SD = 1.76%) or 88.99% (SD = 1.60%) when coupled with Support Vector Machine (SVM). Comparisons were also made with newer methods such as the Minimum Distance to Riemannian Mean (MDRM) that achieved 81.13% (SD = 4.64%) and Tangent Space (TS) 86.09% (SD = 4.48%). All the results issued from the various pipelines are then compared using their confusion matrices. </p> <br><p> More generally, confusion matrices (CM) consist of two independent samples (positive and negative), each following a Binomial distribution. To extend their utility, the confidence intervals (CI) are calculated for the probabilities associated with each sample. A complementary estimation of the minimum sample size is presented based on the Distance Separation (DS) method relying on the chosen CI approximation. DS can serve as a viable alternative to FDMC for experimental design, specifically aimed at evaluating the statistical distinction between two classifiers possessing known accuracies. Finally, a method to estimate the accuracy though standard deviation of CI (SDCI) is presented and applied to visual representations of CM, called Confusion Ellipses (CE). </p> <br><p> Comparisons with PyCM, a Python library dedicated to CM evaluation and comparison, shows that their computations seem to underestimate the CI mean widths around zero compared to SDCI. Various CI approximations are also tested to assess the influence of the CI approximation choice and can serve as reference when designing a new experiment with the goal of comparing two pipelines with known global classification accuracies. The applicability of these novel methodologies (i.e., SDCI, DS, CE) extends far beyond the confines of the BCI field, encompassing a wide range of domains and disciplines. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">guttmannflury2023thesis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Innovative artefact elimination and source localization-based feature extraction for EEG BCI pipelines}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guttmann-Flury, E.}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Shanghai Jiao Tong University}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Brain-Computer Interface (BCI), Electroencephalography (EEG), Spontaneous blinking, Artifact removal, Sample size calculation, Channel selection, Source Localization, eLORETA, Confusion Matrix (CM), Accuracy confidence interval}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Eva Guttmann-Flury. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?d3a25b46bbd2e0a751a27b173abc6e5f"></script> <script defer src="/assets/js/copy_code.js?d359581efc54b08366f9ef8219e6e511" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?66cb7c6fd8c87d040727f1e99683717e"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"> <div class="modal-footer" slot="footer"> <span class="help"> <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 1280 1280"> <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path> </svg> to select </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewbox="0 0 24 24"> <path d="M0 0h24v24H0V0z" fill="none"></path> <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path> </svg> to navigate </span> <span class="help"> <span class="ninja-examplekey esc">esc</span> to close </span> <span class="help"> <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewbox="0 0 20 20" fill="currentColor"> <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path> </svg> move to parent </span> </div> </ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation menu",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Much Ado About Publications",section:"Navigation menu",handler:()=>{window.location.href="/publications/"}},{id:"dropdown-overview",title:"Overview",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blinks",title:"Blinks",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-algorithms",title:"Algorithms",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-variability",title:"Variability",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-statistics",title:"Statistics",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"nav-cv",title:"CV",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation menu",handler:()=>{window.location.href="/cv/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/en-us/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2018/distill/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-blinks",title:"Blinks",description:"A pulsar for EEG signals",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-algorithms",title:"Algorithms",description:"An endless labyrinth of possibilities",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-variability",title:"Variability",description:"A certainty as elusive as a fleeting shadow",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-statistics",title:"Statistics",description:"An unerring compass guiding towards the truth",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"socials-email",title:"Send an email",section:"Socials",handler:()=>{window.open("mailto:%65%76%61.%67%75%74%74%6D%61%6E%6E.%66%6C%75%72%79@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=pamaGFEAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/QinXinlan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/eva-guttmann-flury","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/qinxinlaneva","_blank")}},{id:"lang-zh",title:"zh",section:"Languages",handler:()=>{window.location.href="/zh/publications/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?f74bfa9a88ab862fb9df1c46146b7b7d"></script> </body> </html>